-Dlog4j.configuration=file:sparkRDD/log4j.properties
-Dspark.yarn.app.container.log.dir=sparkRDD/app-logs
-Dlogfile.name=sparkRDD.log


--
From leaern from rockthejvm
---

Spark - It is unified computing engine and distributed data processing framework

Spark API
RDD - Resilient Distributed Dataset


       Spark SQL       DATAFRAME  API      DATASET API

           ---------------------------------------
                       catalyst optimizer
---------------------------------------------------------
                          RDD API

Don't use RDD API directly, use DataFrame or Dataset API. It goes through catalyst optimizer and then RDD API.


Spark SQL - Spark SQL is a Spark module for structured data processing.
            It provides a programming abstraction called DataFrames and can also act as distributed SQL query engine.
            It enables unmodified Hadoop Hive queries to run up to 100x faster on existing deployments and data.
            It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.
            It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.

DataFrame - A DataFrame is a distributed collection of data organized into named columns.
            Generic type.
            Actual data type assign at runtime.
            It supports expression based transformation.
            Dataset[Row]

DataSet API - Language native API in scala and Java.
              Strongly typed API in JVM type object.
              give compile time type safety.
              It supports function based tranfomration.
              Dataset[UD Objetc]

RDD - Resilient Distributed Dataset
    - It is dataset of partitioned data with fault tolerance.
    - Resilient - fault tolerant -
                      - they have information how they are created
                      - if executor fails, drive can recreate RDDs by assigning partitioned data to another executor.



Transfomations:
  1. Narrow Transfomation
           - one input transformation - one output transformation
           - eg. map, filter, union, join, groupByKey, reduceByKey, etc.

  2.Wide Transfomation
            - one input transformation - multiple output transformation
            - eg. groupBy, distinct, sortBy, etc.


Computing dataframe
    1. Lazy evaluation
    2. Planning
         - spark compiles to DF transformation into graph before running any code
         - logical plan - DF dependency graph + narrow/wide transformation sequence
         - physical plan - optimize sequence of steps for nodes in clsuter.
         - optimization -



shuffle - data exchange between executors
          - wide transformation
          - expensive operation


Transformation vs Action
- Transformation - how new dataframe obtained from existing dataframe
- Action - trigger computation and return result



-------------------
1. Actions
2. Transformations
3. Utility Functions

-------------------
Actions
  1.collect() - Return all the elements of the dataset as an array at the driver program.
  2.collectAList() - Return all the elements of the dataset as a list at the driver program.
  3.count() - Return the number of elements in the dataset.
  4.take(n) - Return an array with the first n elements of the dataset.
  5.foreach(f) - Run a function f on each element of the dataset. This is usually done for side effects such as updating an Accumulator or interacting with external storage systems.
  6.reduce(f) - Aggregate the elements of the dataset using a function f (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.
  7.fold(z)(f) - Aggregate the elements of each partition, and then the results for all the partitions, using a given associative function and a neutral "zero value."
  8.aggregate(z)(seqOp, combOp) - Aggregate the elements of each partition, and then the results for all the partitions, using given combine functions and a neutral "zero value."
  9.saveAsTextFile(path) - Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file.

Transformations:
  1. select() - Selects a set of column based on column names or expressions.
  2. groupBy() - Groups the DataFrame using the specified columns, so we can run aggregation on them.
  3. where() - This is equivalent to filter().
  4. repartition() - Returns a new DataFrame partitioned by the given partitioning expressions into numPartitions. The resulting DataFrame is hash partitioned.
  5. map() - Returns a new DataFrame by applying a function to each element of this Dataset.
  6.  flatMap() - Returns a new Dataset by first applying a function to all elements of this Dataset, and then flattening the results.
  7. groupByKey() - When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs.

Utility Functions:
 1. printSchema() - Prints out the schema in the tree format.
 2. cache() - Persist this Dataset with the default storage level (MEMORY_AND_DISK).
 3  persist() - Persist this Dataset with the default storage level (MEMORY_AND_DISK).
 4. unpersist() - Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
 5. createOrReplaceTempView() - Creates or replaces a local temporary view with this Dataset.
 6. rdd() - Returns the content of the Dataset as an RDD of Rows.
 7. write() - Interface for saving the content of the non-streaming Dataset out into external storage.


----------------------------------------
DataSources:
Reading DF
  - format
  - schema (optional)/ information
  - mode  // failFast, dropMalformed, permissive
  - path
  - load

Write DF
   - format
   - mode
   - path
   - save

write DF optional
   allowSingleQuotes
   Compression // snappy, gzip, lzo, etc.


--------------------------------------------------
spark Data types:
   --- org.apache.spark.sql.datatypes._

   -StringType
   -IntegerType
   -StructType
   -StructField
   -ArrayType
      Supported function to arraytype
       - array
       - array_contains
       - element_at
       - explode
       - flatten
       - posexplode
       - split
       - size
       - sort_array
\
      -
   -MapType
   Supported function to MapType
         - map
         - map_keys
         - map_values
         - map_concat
         - map_from_arrays
         - map_from_entries
         - map_keys
         - map_values
         - map_zip_with
         - map_zip_with_index


   -LongType
   -DoubleType
   -FloatType
   -ShortType
   -ByteType
   -BinaryType
   -BooleanType
   -DateType
   Supported function to dateType
      - current_date
        - date_add
        - date_format
        - date_sub
        - datediff
        - dayofmonth
        - dayofweek
        - dayofyear
        - from_unixtime
        - from_utc_timestamp
        - last_day
        - months_between
        - next_day
        - quarter
        - to_date
        - to_utc_timestamp
        - trunc
        - unix_timestamp
        - weekofyear
        - year
        - month
        - day
        - hour
        - minute
   -TimestampType
   -DecimalType
   -NullType
   -CalendarIntervalType
   -NumericType
   -IntegerType



-----------------------
Aggregation function: [Wide transfomation]
    - groupBy, agg, count, sum, avg, min, max, mean, pivot, rollup, cube, etc.
     - stdDev, variance, collect_list, collect_set, first, last, etc.: =

------------------------
joins [Wide transformation]
    - inner       => inner join
    - left_outer  => left join
    - right_outer => right join
    - full_outer  => full join
    - cross       => cross join
    - left_semi   => semi join
    - left_anti   => anti join
    - natural     => natural join


Adding plain value to DF
    - lit() function


 Creating new column
    - from expression of one column
    - from 2,3 column                  ----> we can use udf function


Managing date and time
   - date_add
    - date_sub
    - datediff
    - add_months
    - months_between
    - next_day
    - last_day
    - trunc


Managing nulls-
    - coalesce
    - isNull
    - isNotNull
    - na.fill
    - na.replace
    - na.drop
    - na.dropDuplicates
    - na.dropDuplicates
    - na.drop
    - na.fill


 -----------------------------
 when to use dataset and dataframe
    - when we need compile time type safety, we should use dataset
    - when we need to use functional programming, we should use dataset
    - when we need to use sql query
    - filter transformation hard to express in sql or dataframe

Avoid when
  - perfomance is critical: spark can't optimize datase



  ----------
  RDD - Resilient Distributed Dataset
    first citizen of spark, all higher level API build on top of RDD

  Pro - can be highly optimized
  partitioning can be controlled
  order of data can be controlled
  order of operation matters for performance

  Con - no compile time type safety
  - no optimization
  - no catalyst optimizer

  rdd to DF  --> loose type information
  rdd to DS  --> keep type information


  rdd and dataset common
    - map, filter, flatMap, groupByKey, reduceByKey, join, etc.
    - union,count, distinct, etc.
    - groupBy, sortBy, etc.

  RDD over datasets
    - when we need to control partitioning - repartition, coalesce, partitionBy, zipPartitions, etc.
    - operation control -checkpoint, persist, cache, etc.



--------
Following are important to understand spark DAG

spark has job
job has stage
stage has task

What is stage? --> a set of computation between shuffles
task           ---> a unit of computation per partition


------------------
The anatomy of cluster:
Spark cluster manager (Standalone, YARN, Mesos)
 - one node manages state of cluster
 - other do work
 - communicate via driver worker process

Spark driver
 - manages state of stages/tasks of application
 - interface with cluster manager

Spark executor
  - run task assigned by spark driver
  - return the state and result to driver



Execution mode
  1. cluster
  2. client
  3. local

 Cluster mode
    - spark driver launch on worker node
    - cluster manager responsible for spark process

 Client mode
   - spark driver is on client machine
   - client responsible for spark process and state management

 Local mode:
   -  entire application runs on same machine




